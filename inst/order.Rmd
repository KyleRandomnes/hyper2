---
title: "Order statistics and numerical optimization"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## The `hyper2` package

We consider the likelihood function given by a single observation of
an order statistic.  Here we investigate some numerical problems
arising from the use of `optim()` and `constrOptim()`.  We will
consider $n=10$ competitors and without loss of generality, the
competitors finish in order $1,2,3,\ldots,10$.  The likelihood
function for this observation is:

\[
{\mathcal L}\left(p_1,\ldots,p_{10}\right)=
\frac{p_1}{p_1+\cdots+p_{10}}\cdot
\frac{p_2}{p_2+\cdots+p_{10}}\cdot
\frac{p_3}{p_3+\cdots+p_{10}}\cdot
\frac{p_4}{p_4+\cdots+p_{10}}\cdot
\frac{p_5}{p_5+\cdots+p_{10}}\cdot\\
\frac{p_6}{p_6+\cdots+p_{10}}\cdot
\frac{p_7}{p_7+\cdots+p_{10}}\cdot
\frac{p_8}{p_8+p_9+p_{10}}\cdot
\frac{p_9}{p_9+p_{10}}\cdot
\frac{p_{10}}{p_{10}}
\]

We can calculate the evaluate (that is, the maximum likelihood
estimate) from the formula by inspection.  It is clear from the
penultimate term that $p_{10}=0$.  Similarly from the preceding term
it is easy to see that $p_9=0$ and so on.  The evaluate is thus
$p=(1,0,\ldots,0)$.  Let's see if we can reproduce this using
numerical methods.  Note that this is a highly pathological case; the
likelihood is not defined at the evaluate.  Even considering limiting
cases is not straightforward; consider two processes, approaching the
evaluate in two different ways:

\[
\lim_{p\longrightarrow 0}
{\mathcal L}(1-9p,p,p,p,p,p,p,p,p,p,p)=\frac{1}{8!}
\]

but 

\[
\lim_{p\longrightarrow 0}
{\mathcal L}(\alpha,\alpha p,\alpha p^2,\ldots,\alpha p^9)=1
\]

where $\alpha=(1-p^{10})/(1-p)$ is a normalizing constant.  Thus the
likelihood _at_ the evaluate is not defined, even as a limit (because
it depends on the direction of approach).

The `hyper2` idiom for creating a likelihood function is as follows:

```{r defineH}
library("hyper2")
H <- order_likelihood(rbind(1:10))
H
```
And then maximizing it is carried out using the `maxp()` function:
	

```{r}
evaluate1 <- maxp(H)
(maxlike1 <- loglik(indep(evaluate1),H))
dotchart(evaluate1,pch=16)
```

Comparing the numerically determined maximum likelihood estimate with
the exact MLE of $(1,0,\ldots,0)$ shows that the numerics have not
done a particularly good job in identifying the evaluate, presumably
as a result of the difficulty of honouring the constraints.  However,
if we give the system a hand by starting at a point closer to the
evaluate we can do much better:

```{r}
f <- function(p,n){c(p,rep((1-p)/n,n-1))}
sss <- f(0.9999,9)  # sensible start point, close (?) to the evaluate
evaluate2 <- maxp(H,startp=sss)
maxlike2 <- loglik(indep(evaluate2),H)
```

Graphically:

```{r}
dotchart(evaluate2,pch=16)
```

So it is much better than the coldstart above in the sense that there
is more probability on $p_1$ and less everywhere else.  We can compare
the hotstart above to the coldstart:

```{r}
maxlike2-maxlike1
```

This is negative: the so-called "sensible start point" has produced an
inferior solution.  This is because `sss` has $p_2=p_3=\ldots =
p_{10}$, so predicts a draw between players $2,3,\ldots 10$, contrary
to observation.



A better way is to mimic the more appropriate of the two limiting processes above:

```{r}
vsssp <- function(x,n){
  jj <- 1/x^seq_len(n)
  indep(jj/sum(jj))
}
```

Then

```{r}
evaluate3 <- maxp(H,vsssp(4,10))
maxlike3 <- loglik(indep(evaluate3),H)
dotchart(evaluate3,pch=16)
```

Observe that the likelihood at this point is marginally better than
the `equalp()` start point:


```{r}
maxlike3-maxlike2
```

but the dotchart again shows a quite a lot of probability on
$p_2,\ldots p_{10}$.


# Discussion

I am not sure how important these considerations are in practice.
Firstly, the order statistic used here is extremely pathological, and
perhaps it is unreasonable to expect any inferential tool to work
under these circumstances.

Secondly, and perhaps more importantly, likelihood is not defined at a
point; likelihood is an equivalence class of functions whose domain is
possible hypotheses.  We can only compare likelihoods for two
different hypotheses [because likelihood is only defined up to a
multiplicative constant].  And for the observation considered here,
the evaluate $(1,0,\ldots,0)$ is not a hypothesis anyone would want a
likelihood for: the likelihood is not defined at that point, and does
not even have a well-defined limit there.

To some extent this is not surprising: the numerics force one to have
a strictly positive minimum value for each $p_i$ and the fillup value
$p_{10}=1-\sum p_i$.  The likelihood maximization routine tries to
maximize $p_9/p_{10}, p_8/p_9,\ldots,p_1/p_2$ simultaneously, all
subject to the unit sum constraint $\sum p_i=1$.  And of course, if
the ratio is too high then the unit sum constraint would be violated
(given that $p_{10}$ is at its minimum value).
